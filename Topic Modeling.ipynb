{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5df47d-7415-480d-bb0c-d2ae0dd6f774",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e0824b-6b06-42ff-bfd5-189430af6239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d49ce7-3c89-4d0c-80a1-5f13edf601fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d165874-b060-4ef8-96f4-78940e217ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/topic_dataset_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b61b81d2-ba9d-4dcf-97f5-c5cfeb38db8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>ticker</th>\n",
       "      <th>topic_label</th>\n",
       "      <th>engagement</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>topic_0</td>\n",
       "      <td>2025-02-01T12:16:33.957527</td>\n",
       "      <td>user_889</td>\n",
       "      <td>$NVDA launch reported — investors cautious</td>\n",
       "      <td>NVDA</td>\n",
       "      <td>product_launch</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>topic_1</td>\n",
       "      <td>2025-04-10T12:16:33.957527</td>\n",
       "      <td>user_1234</td>\n",
       "      <td>$JPM insider sell reported — market reaction m...</td>\n",
       "      <td>JPM</td>\n",
       "      <td>insider_trading</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>topic_2</td>\n",
       "      <td>2025-05-07T12:16:33.957527</td>\n",
       "      <td>user_4</td>\n",
       "      <td>$TSLA filing reported — market reaction muted</td>\n",
       "      <td>TSLA</td>\n",
       "      <td>insider_trading</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>topic_3</td>\n",
       "      <td>2025-08-09T12:16:33.957527</td>\n",
       "      <td>user_1597</td>\n",
       "      <td>$AAPL gdp reported — stock jumps</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>macro</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>topic_4</td>\n",
       "      <td>2024-12-01T12:16:33.957527</td>\n",
       "      <td>user_563</td>\n",
       "      <td>$MSFT layoff reported — investors cautious</td>\n",
       "      <td>MSFT</td>\n",
       "      <td>hiring</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                   timestamp       user  \\\n",
       "0  topic_0  2025-02-01T12:16:33.957527   user_889   \n",
       "1  topic_1  2025-04-10T12:16:33.957527  user_1234   \n",
       "2  topic_2  2025-05-07T12:16:33.957527     user_4   \n",
       "3  topic_3  2025-08-09T12:16:33.957527  user_1597   \n",
       "4  topic_4  2024-12-01T12:16:33.957527   user_563   \n",
       "\n",
       "                                                text ticker      topic_label  \\\n",
       "0         $NVDA launch reported — investors cautious   NVDA   product_launch   \n",
       "1  $JPM insider sell reported — market reaction m...    JPM  insider_trading   \n",
       "2      $TSLA filing reported — market reaction muted   TSLA  insider_trading   \n",
       "3                   $AAPL gdp reported — stock jumps   AAPL            macro   \n",
       "4         $MSFT layoff reported — investors cautious   MSFT           hiring   \n",
       "\n",
       "   engagement  \n",
       "0           6  \n",
       "1          10  \n",
       "2           5  \n",
       "3           8  \n",
       "4           2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bddab311-b556-45d3-a15b-6c75f5980778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define custom stop words from the generator template\n",
    "custom_stop_words = [\n",
    "    'reported', 'market', 'reaction', 'muted', 'stock', 'jumps',\n",
    "    'investors', 'cautious', 'analysts', 'note'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a140dd-e397-4c4b-803a-1bcc9ebfbfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b. Combine with standard English stop words\n",
    "stop_words = set(ENGLISH_STOP_WORDS).union(custom_stop_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66459cd0-510f-4aba-9337-3f886c99f37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c. Create a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Remove tickers like $AAPL\n",
    "    text = re.sub(r'\\$\\w+', '', text)\n",
    "    # Remove punctuation and numbers, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Tokenize and remove stop words\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    # Join back into a string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "641519a9-ebdf-4ca1-abb0-8a0b9ba6133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preprocessing Complete ---\n",
      "Example of cleaned text:\n",
      "                                                text  cleaned_text\n",
      "0         $NVDA launch reported — investors cautious        launch\n",
      "1  $JPM insider sell reported — market reaction m...  insider sell\n",
      "2      $TSLA filing reported — market reaction muted        filing\n",
      "3                   $AAPL gdp reported — stock jumps           gdp\n",
      "4         $MSFT layoff reported — investors cautious        layoff\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2d. Apply the function to the 'text' column\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"--- Preprocessing Complete ---\")\n",
    "print(\"Example of cleaned text:\")\n",
    "print(df[['text', 'cleaned_text']].head())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebebc3a-ebe4-48c2-8b8a-c21e12465201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24abd564-7024-4b49-ae4c-2f0f1c07e972",
   "metadata": {},
   "source": [
    "## TF-IDF + NMF\n",
    "\n",
    "We'll vectorize the texts with TF-IDF and run NMF to extract topics.\n",
    "We will print the top words for each topic and assign a dominant topic to each document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ac8fdd5-9ea2-446f-83ce-b8582d4f67be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eaef544-ceb5-4485-a94e-d09a5e7d8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TF-IDF Vectorization ---\n",
    "# We removed stop_words='english' here because we handled it in our custom function\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21fcf27b-f359-4d47-abff-bc7e656b8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT ON THE NEW 'cleaned_text' COLUMN\n",
    "W = vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b993cca-e1fe-43b8-b29f-287156e80166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Topic Modeling (NMF) ---\n",
    "\n",
    "# Set n_components to 10 (the number of original topics)\n",
    "n_topics = 10\n",
    "model = NMF(n_components=n_topics, init='nndsvd', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ca9cf1f-b96d-43ea-9aa7-da4c4a907e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Chetan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "# model.fit(W) # This is just fitting, use fit_transform to get W_topics\n",
    "W_topics = model.fit_transform(W) # Document-topic matrix\n",
    "H_topics = model.components_       # Topic-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c0128c7-e45c-4fce-b688-40b20577e5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top Words (After Cleaning) ---\n",
      "Topic 0: insider, sell, lawsuit, logistics, cpi, yield, gdp, merger, inflation, acquire\n",
      "Topic 1: supplier, released, regulatory, recession, yield, forecast, estimates, filing, fine, form\n",
      "Topic 2: hiring, shareholder, estimates, form, fine, recruitment, gdp, rollout, yield, economy\n",
      "Topic 3: compliance, rollout, new, product, forecast, merger, spinoff, buyout, guidance, gdp\n",
      "Topic 4: probe, recruitment, logistics, headcount, delay, guidance, outlook, beat, yield, filing\n",
      "Topic 5: port, fine, dividend, released, deal, inflation, acquire, earnings, eps, gdp\n",
      "Topic 6: quarter, form, delay, buyback, deal, launch, spinoff, acquire, earnings, revenue\n",
      "Topic 7: filing, guidance, launch, payout, layoff, return, cash, forecast, recession, hike\n",
      "Topic 8: unveiled, shortage, chip, released, gdp, merger, recession, profits, spinoff, inflation\n",
      "Topic 9: talent, buyback, estimates, cash, return, forecast, lawsuit, regulatory, integration, yield\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Display Results ---\n",
    "\n",
    "def print_top_words(H, feature_names, n_top_words=10):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        top_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
    "        top_words = [feature_names[i] for i in top_indices]\n",
    "        print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n",
    "\n",
    "print(\"--- Top Words (After Cleaning) ---\")\n",
    "print_top_words(H_topics, feature_names, n_top_words=10)\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1814a6d5-0d1c-413b-ae21-80eb2e68a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Assign Topics to Documents ---\n",
    "dominant_topics = W_topics.argmax(axis=1)\n",
    "df['pred_topic'] = dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac9b5fef-c445-463b-b1c6-1f5f4a3c7146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Example Predictions (After Cleaning) ---\n",
      "                                                   text cleaned_text  \\\n",
      "6252    $NVDA earnings reported — market reaction muted     earnings   \n",
      "4684         $BAC lawsuit reported — investors cautious      lawsuit   \n",
      "1731             $MSFT lawsuit reported — analysts note      lawsuit   \n",
      "4742              $AMZN buyout reported — analysts note       buyout   \n",
      "4521        $NVDA economy reported — investors cautious      economy   \n",
      "6340       $GOOG dividend reported — investors cautious     dividend   \n",
      "576                     $JPM gdp reported — stock jumps          gdp   \n",
      "5202  $MSFT shareholder reported — market reaction m...  shareholder   \n",
      "6363               $AMZN acquire reported — stock jumps      acquire   \n",
      "439      $AMZN outlook reported — market reaction muted      outlook   \n",
      "\n",
      "     topic_label  pred_topic  \n",
      "6252    earnings           8  \n",
      "4684  regulation           9  \n",
      "1731  regulation           9  \n",
      "4742      merger           7  \n",
      "4521       macro           9  \n",
      "6340    dividend           5  \n",
      "576        macro           8  \n",
      "5202    dividend           2  \n",
      "6363      merger           6  \n",
      "439     guidance           4  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"--- Example Predictions (After Cleaning) ---\")\n",
    "# Show original text, cleaned text, predicted topic, and true label\n",
    "print(df[['text', 'cleaned_text', 'topic_label', 'pred_topic']].sample(10, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff42b9a-d1bc-4ce2-9bd4-fef3b3b7f2de",
   "metadata": {},
   "source": [
    "# logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06f1765c-e47c-4899-941c-750c0c5ec006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIT ON THE NEW 'cleaned_text' COLUMN\n",
    "X = vectorizer.fit_transform(df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1427f8b1-df49-4bea-8df0-c26ad5e14139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Define Labels (y) ---\n",
    "y = df['topic_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ff5d64-272a-4256-877d-04b7308a338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Train-Test Split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66154c59-cf1d-493c-a714-8ba61022286a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 8000\n",
      "Test set size: 2000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "007ae218-7c37-41e9-b41a-1c621e59c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6dc16450-a1a5-463d-af8a-b5c751601969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f638ab3a-6da9-4295-a914-afb94ec167d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Evaluate Model ---\n",
    "print(\"--- Model Evaluation ---\")\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8e14523-f1fd-49e7-8610-d31b0959f475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06e0d571-9226-4078-a3c7-10c3c7c60ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       dividend       1.00      1.00      1.00       197\n",
      "       earnings       1.00      1.00      1.00       200\n",
      "       guidance       1.00      1.00      1.00       193\n",
      "         hiring       1.00      1.00      1.00       198\n",
      "insider_trading       1.00      1.00      1.00       195\n",
      "          macro       1.00      1.00      1.00       205\n",
      "         merger       1.00      1.00      1.00       204\n",
      " product_launch       1.00      1.00      1.00       204\n",
      "     regulation       1.00      1.00      1.00       202\n",
      "   supply_chain       1.00      1.00      1.00       202\n",
      "\n",
      "       accuracy                           1.00      2000\n",
      "      macro avg       1.00      1.00      1.00      2000\n",
      "   weighted avg       1.00      1.00      1.00      2000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show detailed report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c10e551e-516c-4f68-a0ce-ac8b944c6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the original text for the test set examples\n",
    "test_indices = y_test.index\n",
    "example_df = df.loc[test_indices].copy()\n",
    "example_df['predicted_label'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83bfdb42-2405-4825-8890-f5f98feecb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  text cleaned_text  \\\n",
      "6553                $BAC layoff reported — stock jumps       layoff   \n",
      "6042               $AAPL Form 4 reported — stock jumps         form   \n",
      "215                 $BAC deal reported — analysts note         deal   \n",
      "7775  $META estimates reported — market reaction muted    estimates   \n",
      "3738    $AMZN spinoff reported — market reaction muted      spinoff   \n",
      "4781                 $AMZN fine reported — stock jumps         fine   \n",
      "4868                 $MSFT port reported — stock jumps         port   \n",
      "104          $GOOG compliance reported — analysts note   compliance   \n",
      "6787    $BAC new product reported — investors cautious  new product   \n",
      "2018   $AMZN shareholder reported — investors cautious  shareholder   \n",
      "\n",
      "          topic_label  predicted_label  \n",
      "6553           hiring           hiring  \n",
      "6042  insider_trading  insider_trading  \n",
      "215            merger           merger  \n",
      "7775         guidance         guidance  \n",
      "3738           merger           merger  \n",
      "4781       regulation       regulation  \n",
      "4868     supply_chain     supply_chain  \n",
      "104        regulation       regulation  \n",
      "6787   product_launch   product_launch  \n",
      "2018         dividend         dividend  \n"
     ]
    }
   ],
   "source": [
    "# Display examples\n",
    "print(example_df[['text', 'cleaned_text', 'topic_label', 'predicted_label']].sample(10, random_state=42))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
